%%=============================================================================
%% Methodologie
%%=============================================================================

\chapter{\IfLanguageName{dutch}{Methodologie}{Real-world solutions with Quantum}}
\label{ch:computing-with-quantum}

Once we started looking into quantum theory and everything it could possibly encompass for our scientific project, we found ourselves in one of the deepest rabbit holes we could have possibly found. The true value of quantum research is how we can actually use it for real-life solutions. One could easily imagine that being able to simulate an exact medicine within a couple of days instead of the many months it takes at the moment, would save a numerous amount of lives. So keeping this same train of thought throughout, it is of great importance that we actually focus our attention on what current developments could possibly mean for existing projects and research.

\subsection{Quantum computing and traditional computing}

QC is and will never be the sole solution to a problem. This new form of computing is made to be an addition to points where classical computing fails, e.g. searching through an extremely large dataset without having a clear index within a polynomial time frame such as in \textcite{Terhal1998}. QC also has its limits as it takes a lot longer to actually set up your computation than it would take on a regular machine. But it could be able to solve a couple of non polynomial problems we are currently facing in computer science like factorization. 
Some problems have been left NP-complete (non-polynomial) even with quantum attempts like this paper has tried, \textcite{Wang2007}. Quantum computing must not be looked at as the single solution for every problem, it still needs the help of classical computing to be able to perform its more advantageous tasks. 

Classical computing is great at organising and shuffling data around and performing parallel actions on your device, but with the help of QC we would be able to shift the heavy long term calculations over to devices especially made for long term and hard calculations like a quantum processor. Calculating a machine learning model (\textcite{Schuld2014}) or performing an accurate simulation of a new medicine could be exponentially reduced in time, which would return the value of these calculations to the business side in a much faster way. \autocite{Schuld2015} \autocite{Troyer2005}

\subsection{Quantum computing and the mainframe}

First of all we need to clarify what a mainframe is and what its main use is in our current business environments. A mainframe is a type of supercomputer that is different from other supercomputers because it is not specialised in solving 1 really hard problem, like simulations or factorisation, it is specialised to have the highest possible amount of secure throughput for smaller calculations. The mainframe is widely used within the banking, production and logistical sector as it offers the most reliable way of managing your data that is generated by a certain business practice.

To clarify let us look at an example where a mainframe computer like the IBM Z15 shines. When millions of users throughout the world want to buy their flight tickets towards France around the end of April, a huge bottleneck is created at the end point of the booking system of the particular airport. A mainframe handles these types of atomic transactions quickly to make sure every single booking will come through with the correct data. If the data were to be corrupted along the way, the mainframe would be able to spot out these irregularities and discard this data so that the user receives a proper notification as soon as possible. So look at a mainframe computer as a really good processor of input and output of small tasks.

To quickly compare, a normal super computer is used because of its high-speed processors that are able to create simulations, ML-models... They are designed to perform one synchronous task really well and if necessary brute force this task.

IBM has released the new mainframe Z15 in 2019, with a broad future perspective, because as one of the top researchers in quantum technology they have a clear image of how a quantum computer could influence themselves and others within their sector.
They are emphasising on 2 very different aspects to make sure their devices are the most likely to take the biggest market share, modernisation and security. 
With modernisation IBM is trying the incorporate the mainframe in as much areas as possible to keep on attracting new developers so that their devices don't fall behind. With this modernisation a lot of opportunities are opening up to connect widely different departments such as Machine learning and quantum research.

They have also emphasized on creating new security measures which focusses more on digital signing compared to the current RSA factorisation algorithm. This could secure the mainframe security status indefinitely. Quantum would in the future indeed be able to brute force these RSA based algorithms (\textcite{Shor2000}) and that is why data-security has become such a high importance area at the moment for everyone in computer science that knows the potential hazard of powerful quantum devices.

So now that you are able to view what role the mainframe plays, we can more clearly look at how quantum computers could offer major benefits as a complementary service for solving the harder problems just like a super computer works with the mainframe in a similar way. Nowadays all the data generated from the billions of transactions from the mainframe are preserved so that afterwards a supercomputer would be able to process all this information inside a reasonable time frame to get the best possible business value out of it. If the quantum computer would be able to help process this data exponentially faster, the business value of this data would also exponentially increase. Mainframes offer this great amount of throughput of data that could offer this exponential force of QC the data-items it needs to become viable for business. Again this is something that could show how well QC can fit in our existing model of computation, to further enhance the processes that drive our economies. QC will become the most helpful ally to make sure all our classical systems become even more valuable.

Mainframes are here to stay, with the dawn of fully data-driven worlds that operate on an autonomous platform and if QC could add to such an industry important device it will most definitely boost both devices in exposure and value. But for now there most certainly are questions of when QC would be able to reliably process and/or handle the high throughput of a mainframe in a manner that would add to its business value. This will form another benchmark for QC where full cooperation of these devices can transform into something practical and valuable.




\subsection{Quantum computing and Machine Learning}

Another area where QC could have a major impact is the area of Machine Learning \textbf{(ML)}. At this moment machine learning is running into a bottleneck where the amount of data has become so intense that ordinary classical computers are not able to process the data in time so that its value can be exploited to its maximum potential. QC could help with this issue in a couple of major aspects, like data model training and data capturing. This would greatly improve the impact of ML on the business side, because the relay of the captured information through the models could indeed be shortened in exponential ways. \autocite{Biamonte2017}

At this moment research is becoming quite prevalent in ML with a combination of QC-technology. Qiskit has also seen this opportunity opening up and they too try and attract businesses with these advantages. The current research groups at IBM and Google are able to enhance supervised learning algorithms as well as unsupervised learning, with time series or without. Algorithms such as linear regression, k-means clustering and even neural networks can be enhanced during its training phases with QC. Due to superposition and entanglement, these algorithms could train a model theoretically through one loop instead of having multiple epochs that contain a certain batch size, which obviously speeds up the generation of models that require a large amount of data to become valuable.

The utilisation of QC with ML would not aid the accuracy of ML in the short run because of the uncertainty of solving the quantum decoherence issue for now. But the time frame of processing a complete machine learning model could eventually be exponentially decreased.





